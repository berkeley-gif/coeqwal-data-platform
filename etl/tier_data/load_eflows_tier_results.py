#!/usr/bin/env python3
"""
ETL script to load Environmental Flows (ENV_FLOWS) tier results from CSV.

Transforms Eflows_Tier_Results.csv into:
1. tier_location_result table records (individual location tiers)
2. tier_result table records (aggregate tier counts)

Usage:
    # Dry run (preview SQL)
    python load_eflows_tier_results.py --dry-run
    
    # Generate SQL file
    python load_eflows_tier_results.py --output-sql eflows_tiers.sql
    
    # Load directly to database
    DATABASE_URL=postgres://... python load_eflows_tier_results.py
"""

import argparse
import os
import sys
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional
from collections import Counter

# Location ID to name mapping (from existing tier_location_result data)
LOCATION_NAMES = {
    'AMR004': 'American River at I-80 Bridge',
    'FTR003': 'Feather River',
    'FTR029': 'Feather River at Yuba City',
    'MCD005': 'Merced River at Stevinson',
    'MOK028': 'Mokelumne River',
    'SAC000': 'Sacramento at confluence',
    'SAC049': 'Sacramento River at Freeport',
    'SAC122': 'Sacramento River at Tisdale Weir',
    'SAC148': 'Sacramento River at Colusa Weir',
    'SAC257': 'Sacramento River above Bend Bridge',
    'SAC289': 'Sacramento River (South Bonnieville)',
    'SJR070': 'San Joaquin near Vernalis',
    'SJR127': 'San Joaquin at Salt Slough',
    'STS011': 'Stanislaus River',
    'TRN111': 'Trinity River at Lewiston',
    'TUO003': 'Tuolumne River',
    'YUB002': 'Yuba River at Marysville',
}

# Display order (matching existing data)
DISPLAY_ORDER = {
    'AMR004': 1,
    'FTR003': 2,
    'FTR029': 3,
    'MCD005': 4,
    'MOK028': 5,
    'SAC000': 6,
    'SAC049': 7,
    'SAC122': 8,
    'SAC148': 9,
    'SAC257': 10,
    'SAC289': 11,
    'SJR070': 12,
    'SJR127': 13,
    'STS011': 14,
    'TRN111': 15,
    'TUO003': 16,
    'YUB002': 17,
}

TIER_SHORT_CODE = 'ENV_FLOWS'
LOCATION_TYPE = 'network_node'


def load_csv(csv_path: str) -> pd.DataFrame:
    """Load and validate the Eflows tier results CSV."""
    df = pd.read_csv(csv_path, index_col=0)
    
    print(f"Loaded {csv_path}")
    print(f"  Locations: {len(df)} rows")
    print(f"  Scenarios: {list(df.columns)}")
    
    # Validate location IDs
    unknown_locations = set(df.index) - set(LOCATION_NAMES.keys())
    if unknown_locations:
        print(f"  WARNING: Unknown location IDs: {unknown_locations}")
    
    return df


def transform_to_location_results(df: pd.DataFrame) -> List[Dict]:
    """Transform CSV data to tier_location_result format."""
    results = []
    
    for location_id in df.index:
        for scenario_col in df.columns:
            tier_level = int(df.loc[location_id, scenario_col])
            
            results.append({
                'scenario_short_code': scenario_col,
                'tier_short_code': TIER_SHORT_CODE,
                'location_type': LOCATION_TYPE,
                'location_id': location_id,
                'location_name': LOCATION_NAMES.get(location_id, location_id),
                'tier_level': tier_level,
                'tier_value': 1,  # Each location counts as 1
                'display_order': DISPLAY_ORDER.get(location_id, 99),
            })
    
    return results


def calculate_tier_aggregates(df: pd.DataFrame) -> List[Dict]:
    """Calculate aggregate tier counts per scenario for tier_result table."""
    results = []
    
    for scenario_col in df.columns:
        tier_counts = Counter(df[scenario_col].astype(int))
        total = len(df)
        
        # Calculate normalized values (percentage as decimal)
        results.append({
            'scenario_short_code': scenario_col,
            'tier_short_code': TIER_SHORT_CODE,
            'tier_1_value': tier_counts.get(1, 0),
            'tier_2_value': tier_counts.get(2, 0),
            'tier_3_value': tier_counts.get(3, 0),
            'tier_4_value': tier_counts.get(4, 0),
            'norm_tier_1': round(tier_counts.get(1, 0) / total, 4) if total > 0 else 0,
            'norm_tier_2': round(tier_counts.get(2, 0) / total, 4) if total > 0 else 0,
            'norm_tier_3': round(tier_counts.get(3, 0) / total, 4) if total > 0 else 0,
            'norm_tier_4': round(tier_counts.get(4, 0) / total, 4) if total > 0 else 0,
            'total_value': total,
            'single_tier_level': None,  # multi_value tier
        })
    
    return results


def generate_location_result_sql(location_results: List[Dict]) -> str:
    """Generate SQL for tier_location_result upsert."""
    lines = [
        "-- ENV_FLOWS Tier Location Results",
        "-- Generated by load_eflows_tier_results.py",
        "",
        "INSERT INTO tier_location_result (",
        "    scenario_short_code, tier_short_code, location_type, location_id,",
        "    location_name, tier_level, tier_value, display_order",
        ") VALUES"
    ]
    
    values = []
    for r in location_results:
        values.append(
            f"    ('{r['scenario_short_code']}', '{r['tier_short_code']}', "
            f"'{r['location_type']}', '{r['location_id']}', "
            f"'{r['location_name']}', {r['tier_level']}, {r['tier_value']}, {r['display_order']})"
        )
    
    lines.append(',\n'.join(values))
    lines.append("ON CONFLICT (scenario_short_code, tier_short_code, location_id, tier_version_id)")
    lines.append("DO UPDATE SET")
    lines.append("    tier_level = EXCLUDED.tier_level,")
    lines.append("    tier_value = EXCLUDED.tier_value,")
    lines.append("    location_name = EXCLUDED.location_name,")
    lines.append("    display_order = EXCLUDED.display_order,")
    lines.append("    updated_at = NOW(),")
    lines.append("    updated_by = coeqwal_current_operator();")
    lines.append("")
    
    return '\n'.join(lines)


def generate_tier_result_sql(tier_aggregates: List[Dict]) -> str:
    """Generate SQL for tier_result upsert."""
    lines = [
        "-- ENV_FLOWS Tier Result Aggregates",
        "-- Generated by load_eflows_tier_results.py",
        "",
        "INSERT INTO tier_result (",
        "    scenario_short_code, tier_short_code,",
        "    tier_1_value, tier_2_value, tier_3_value, tier_4_value,",
        "    norm_tier_1, norm_tier_2, norm_tier_3, norm_tier_4,",
        "    total_value, single_tier_level",
        ") VALUES"
    ]
    
    values = []
    for r in tier_aggregates:
        values.append(
            f"    ('{r['scenario_short_code']}', '{r['tier_short_code']}', "
            f"{r['tier_1_value']}, {r['tier_2_value']}, {r['tier_3_value']}, {r['tier_4_value']}, "
            f"{r['norm_tier_1']}, {r['norm_tier_2']}, {r['norm_tier_3']}, {r['norm_tier_4']}, "
            f"{r['total_value']}, NULL)"
        )
    
    lines.append(',\n'.join(values))
    lines.append("ON CONFLICT (scenario_short_code, tier_short_code, tier_version_id)")
    lines.append("DO UPDATE SET")
    lines.append("    tier_1_value = EXCLUDED.tier_1_value,")
    lines.append("    tier_2_value = EXCLUDED.tier_2_value,")
    lines.append("    tier_3_value = EXCLUDED.tier_3_value,")
    lines.append("    tier_4_value = EXCLUDED.tier_4_value,")
    lines.append("    norm_tier_1 = EXCLUDED.norm_tier_1,")
    lines.append("    norm_tier_2 = EXCLUDED.norm_tier_2,")
    lines.append("    norm_tier_3 = EXCLUDED.norm_tier_3,")
    lines.append("    norm_tier_4 = EXCLUDED.norm_tier_4,")
    lines.append("    total_value = EXCLUDED.total_value,")
    lines.append("    updated_at = NOW(),")
    lines.append("    updated_by = coeqwal_current_operator(),")
    lines.append("    is_active = TRUE;")
    lines.append("")
    
    return '\n'.join(lines)


def append_to_seed_csv(location_results: List[Dict], seed_csv_path: str):
    """Append new records to the seed CSV file."""
    # Read existing
    existing_df = pd.read_csv(seed_csv_path)
    
    # Create new records DataFrame
    new_df = pd.DataFrame(location_results)
    new_df = new_df[['scenario_short_code', 'tier_short_code', 'location_type', 
                     'location_id', 'location_name', 'tier_level', 'tier_value', 'display_order']]
    
    # Check for duplicates
    existing_keys = set(zip(existing_df['scenario_short_code'], 
                           existing_df['tier_short_code'], 
                           existing_df['location_id']))
    
    new_records = []
    for _, row in new_df.iterrows():
        key = (row['scenario_short_code'], row['tier_short_code'], row['location_id'])
        if key not in existing_keys:
            new_records.append(row)
    
    if new_records:
        new_records_df = pd.DataFrame(new_records)
        combined_df = pd.concat([existing_df, new_records_df], ignore_index=True)
        combined_df.to_csv(seed_csv_path, index=False)
        print(f"Added {len(new_records)} new records to {seed_csv_path}")
    else:
        print(f"No new records to add (all already exist)")


def main():
    parser = argparse.ArgumentParser(description='Load Eflows tier results')
    parser.add_argument('--csv-path', default='../pipelines/Eflows_Tier_Results.csv',
                        help='Path to Eflows_Tier_Results.csv')
    parser.add_argument('--dry-run', action='store_true',
                        help='Print SQL without executing')
    parser.add_argument('--output-sql', type=str,
                        help='Write SQL to file instead of executing')
    parser.add_argument('--update-seed', action='store_true',
                        help='Update the seed CSV file')
    
    args = parser.parse_args()
    
    # Resolve paths
    script_dir = Path(__file__).parent
    csv_path = script_dir / args.csv_path
    
    if not csv_path.exists():
        print(f"Error: CSV file not found: {csv_path}")
        sys.exit(1)
    
    # Load and transform data
    df = load_csv(str(csv_path))
    
    location_results = transform_to_location_results(df)
    tier_aggregates = calculate_tier_aggregates(df)
    
    print(f"\nTransformed data:")
    print(f"  Location results: {len(location_results)} records")
    print(f"  Tier aggregates: {len(tier_aggregates)} records")
    
    # Print tier distribution summary
    print(f"\nTier distribution by scenario:")
    for agg in tier_aggregates:
        print(f"  {agg['scenario_short_code']}: "
              f"T1={agg['tier_1_value']}, T2={agg['tier_2_value']}, "
              f"T3={agg['tier_3_value']}, T4={agg['tier_4_value']}")
    
    # Generate SQL
    location_sql = generate_location_result_sql(location_results)
    aggregate_sql = generate_tier_result_sql(tier_aggregates)
    full_sql = location_sql + "\n" + aggregate_sql
    
    if args.dry_run:
        print("\n" + "=" * 60)
        print("DRY RUN - SQL that would be executed:")
        print("=" * 60)
        print(full_sql)
        return
    
    if args.output_sql:
        output_path = script_dir / args.output_sql
        with open(output_path, 'w') as f:
            f.write(full_sql)
        print(f"\nSQL written to: {output_path}")
        return
    
    if args.update_seed:
        seed_csv_path = script_dir.parent.parent / 'database/seed_tables/10_tier/tier_location_result.csv'
        append_to_seed_csv(location_results, str(seed_csv_path))
        return
    
    # Execute against database
    database_url = os.environ.get('DATABASE_URL')
    if not database_url:
        print("\nError: DATABASE_URL environment variable not set")
        print("Use --dry-run, --output-sql, or --update-seed instead")
        sys.exit(1)
    
    try:
        import psycopg2
        
        print(f"\nConnecting to database...")
        conn = psycopg2.connect(database_url)
        cur = conn.cursor()
        
        # Execute location results
        print("Inserting tier_location_result records...")
        cur.execute(location_sql)
        
        # Execute tier aggregates
        print("Inserting tier_result records...")
        cur.execute(aggregate_sql)
        
        conn.commit()
        print(f"Successfully loaded {len(location_results)} location records and {len(tier_aggregates)} aggregate records")
        
        cur.close()
        conn.close()
        
    except ImportError:
        print("Error: psycopg2 not installed. Use --dry-run or --output-sql instead.")
        sys.exit(1)
    except Exception as e:
        print(f"Database error: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()
